<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Fine-tuning LLaMA into a general-purpose instruction follower.">
  <meta name="keywords" content="LLM, PEFT, Instruction Tuning, Multi-modal ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://.github.io">
            SpaceCLIP
          </a>
          <a class="navbar-item" href="https://.github.io">
            VideoDistill
          </a>
          <a class="navbar-item" href="https://.github.io">
            TeethSEG
          </a>
          <a class="navbar-item" href="https://.github.io">
            DFCP
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/favicon.jpg" alt="Publication Icon" class="publication-icon">  
          <h1 class="title is-1 publication-title">LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Bo Zou</a><sup>1</sup>,</span>
            <span class="author-block">
              Chao Yang</a><sup>2</sup>,</span>
            <span class="author-block">
              Yu Qiao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Chengbin Quan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Youjian Zhao</a><sup>1,3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University, </span>
            <span class="author-block"><sup>2</sup>Shanghai AI Laboratory, </span>
            <span class="author-block"><sup>3</sup>Zhongguancun Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming soon!)</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, 
            which introduce extra modules or additional input sequences to inject new skills or knowledge, 
            may compromise the innate abilities of LLMs.
          </p>

          <p>
            In this paper, we propose LLaMA-Excitor, 
            a lightweight method that stimulates the LLMs' potential to better follow instructions 
            by gradually paying more attention to worthwhile information.
          </p>
          
          <p>
            Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state 
            during the self-attention calculation of the transformer structure.
            We designed the Excitor block as a bypass module for the similarity score computation 
            in LLMs' self-attention to reconstruct keys and change the importance of values by a set of learnable prompts. 
            LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, 
            thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-following datasets. 
          </p>

          <p>
            Furthermore, we unify the modeling of multi-modal tuning and language-only tuning, 
            extending LLaMA-Excitor to a powerful visual instruction follower without the need for complex multi-modal alignment. 
          </p>

          <p>
            Our approach is evaluated in language-only and multi-modal tuning experimental scenarios. 
            Compared with LLaMA-7B, LLaMA-Excitor is the only PEFT method that maintains basic capabilities 
            and achieves +3.12% relative improvement on the MMLU benchmark. 
            In the visual instruction tuning, we achieve a new state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, 
            and a comparable performance (88.39%) on ScienceQA to cutting-edge models with more parameters and extensive vision-language pertaining.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered">
      <div class="column is-three-fifths has-text-centered">
        <h2 class="title is-3">Pipeline</h2>
        <p>
          LLaMA-Excitor indirectly involves learnable information in the reasoning process by changing the similarity matrices. 
          It ensures that the hidden states are within the original distribution of LLaMA.
        </p>
        <div class="content has-text-justified">
          <img src="static/images/pipeline.png" alt="Pipeline of LLaMA-Excitor" class="blend-img-background center-image">
        </div>
      </div>
      <div class="column is-one-fourths has-text-centered">
        <h2 class="title is-3">Multi-modal Extention</h2>
        <p>
          LLaMA-Excitor uniformly models multi-modal and language-only tuning and 
          extends language models into powerful vision-language models in a low-budget way.
        </p>
        <div class="content has-text-justified">
          <img src="static/images/multi-modal extention.png" alt="Multi-modal Extention" class="blend-img-background center-image">
        </div>
      </div>
    </div>
    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Qualitative Results</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Textual Instruction-Following</h3>
        <div class="content has-text-justified">
          <p>
            LLaMA-Excitor provides more details and better character identification.  
          </p>
          <img src="static/images/textual instruction-following.png" alt="textual instruction-following" class="blend-img-background center-image">
        </div>
      </div>
    </div>
   
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Visual Instruction-Following</h3>
        <div class="content has-text-justified">
          <p>
            LLaMA-Excitor can accurately cover the content of human annotations and provide richer details
          </p>
          <img src="static/images/visual instruction-following.png" alt="visual instruction-following" class="blend-img-background center-image">
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Quantitative Results</h2>  
    
    <h3 class="title is-4">Image Captioning on MSCOCO</h3>
    <p>
      LLaMA-Excitor significantly surpassing the previous SOTA!  
    </p>
    <div class="columns is-centered">
      <div class="column is-three-fifths">
        <div class="content has-text-justified">
          <img src="static/images/coco.png" alt="textual instruction-following" class="blend-img-background center-image">
        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Question Answering on ScienceQA</h3>
        <div class="content has-text-justified">
          <p>
            LLaMA-Excitor demonstrates comparable performance with SOTAs, although we avoid heavy pretraining for vision-language alignment and massive updating of LLM parameters.
          </p>
          <img src="static/images/sqa.png" alt="textual instruction-following" class="blend-img-background center-image">
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
      anonymous2024llamaexcitor,
      title={{LL}a{MA}-Excitor: General Instruction Tuning via Indirect Feature Interaction},
      author={Anonymous},
      booktitle={Conference on Computer Vision and Pattern Recognition 2024},
      year={2024},
      url={https://openreview.net/forum?id=e7lM7tGXC9}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
